# In the future, it makes sense to generate the county & state
# config from a pre-config script that takes in user input
# for which counties or areas they want to study. Then, 
# the config.yaml is generated from this user input
# in the following structure so that snakemake can do its
# magic. We would want to distribute jobs by 
# nation, state, county so they could run in parallel
FIPS: ["34007"]
STATEFIPS: ["34"]
STATEABBR: ["NJ"]
NATION: ["US"]

# Below are urls and api endpoints
# for obtaining all raw data
# We will organize by county, state
# and national downloads
# And then, within each, we will
# organize by url and api calls
# Finally, we will include key/val pairs
# for component type (i.e. haz, exp, vuln)
# and other identifiers (i.e. social, physical)
# that are useful for organizational logic

# There are different rules for downloading
# data from API or URL endpoints, since the former
# require some parameters whereas the latter
# point straight to a file to download
# We will first have keys
# for api/url 
# Then we will structure by keys
# for the data type (i.e. haz/exp)
# that will define file directories and paths
# These all need to be nested within 
# the following keys
# county, state, national, external
# we want to download data from
# largest to smallest scale (i.e. national, state, county)
# because of their set-type relationships (i.e. many counties in one state
# and many states in one nation)
# Some data downloads are not scale based, so
# downloaded once and from the "external" key
# (this is theoretical, not relevant to our case study)
download:
  FIPS:
    api:
      exp: 
        # NSI has an API endpoint
        # We call it county by county
        nsi: "https://nsi.sec.usace.army.mil/nsiapi/structures?fips={FIPS}"
      ref:
            # We use the Gloucester City, NJ municipal boundary as our clip
            # file for this case study
        clip: "https://services3.arcgis.com/JGF6qCAQFbROcocK/arcgis/rest/services/CamdenCountyMunicipalLayer/FeatureServer/0/query?f=geojson&where=(NAMELSAD%20IN%20(%27Gloucester%20City%20city%27))&outFields=*"
    url:
        # We use the national flood hazard layer for flood zones
        # These URLs currently are not in a structured format, as far as I know
        # and to generalize this may be better suited to a more dynamic
        # speficiation (i.e. maybe with url prefix and then the {fname}.zip specified
        # more dynamically)
        pol:
          nfhl: "https://hazards.fema.gov/nfhlv2/output/County/34007C_20160816.zip"
  STATEABBR:
    url:
      vuln:
        social:
          ovb: "https://www.nj.gov/dep/gis/digidownload/zips/OpenData/Govt_census_group_2022_EJ.zip"
      ref:
        # replace STATE_FIPS with st_fips in the download rule
        # We have a helper function in a util/ directory that replaces
        # Tract, block group, and block geospatial boundaries are available at state level
        tract: "https://www2.census.gov/geo/tiger/TIGER2022/TRACT/tl_2022_{STATEFIPS}_tract.zip"
        bg: "https://www2.census.gov/geo/tiger/TIGER2022/BG/tl_2022_{STATEFIPS}_bg.zip"
  NATION:
    url:
      vuln:
        social:
          # SVI, CEJST and low moderate income designations available for the whole country
          # I'm having trouble downloading lmi from the url endpoint
          # so just downloaded that separately from
          # https://www.hudexchange.info/sites/onecpd/assets/File/ACS_2015_lowmod_blockgroup_all.xlsx
          # and uploaded it the raw/ directory
          svi: "https://svi.cdc.gov/Documents/Data/2020/csv/states/SVI_2020_US.csv"
          cejst: "https://static-data-screeningtool.geoplatform.gov/data-versions/1.0/data/score/downloadable/1.0-communities.csv"
      ref:
        # County for the whole country
        county: "https://www2.census.gov/geo/tiger/TIGER2022/COUNTY/tl_2022_us_county.zip"
# We need to keep track of the format different api data is returned in
# to write it out
api_ext:
  nsi: ".json"
  clip: ".json"

# We need to keep track of which wildcards are used in urls
url_wildcards: ["{FIPS}", "{STATEABBR}", "{STATEFIPS}", "{NATION}"]

# We need to keep track of the hazard directory name
haz_dir_sub: "Combination_Rain_WL"

# Keep track of NSI CRS
nsi_crs: "EPSG:4326"

# Keep track of clip CRS
# The metadata says 26918, but when you
# use that and plot, you clearly see the
# coordinates are in degrees
# Upon further inspection, epsg 4326 appers
# to be the correct CRS
clip_crs: "EPSG:4326"

# Hazard model configurations
# Keep track of return periods
RPs: ["001", "002", "005", "010", "015", "020", "025", "050", "075", "100", "200", "500"]
haz_filename: "{rp}_{scen}.asc"
haz_crs: "EPSG:26918"
scenarios: ['Lower', 'Mid', 'Upper']

# Dictionary of reference filenames to 
# what we want them to say
ref_names:
  tract: "tract"
  bg: "bg"
  county: "county"

# Dictionary of ref names to their
# id column
ref_id_names:
  tract: "GEOID"
  bg: "GEOID"

# Constants for loss estimation
# Uncertainty for structure values
# State of the art automated valuation models
# are generally mean unbiased
# This paper https://www.tandfonline.com/doi/full/10.1080/09599916.2020.1807587
# suggests that a linear model can capture ~ 40% of structure value
# estimates by w/in 10% of their value, and ~ 83% of structure value estimates
# by w/in 30% of their value
# We don't have guidance on what
# the accuracy of NJ assessments are
# We also don't have guidance on how well the
# NSI structure values are calibrated. They are modeled from
# a non-transparent process and do not come with uncertainty
# or bias estimates
# If we use guidance from the Krause et al. paper linked
# above, we can define a normal distribution that is centered
# at the predicted value and has a standard deviation
# equal to predicted value * .2. At the min structure value
# in our case study, $50K, if you 
# look at the probability of getting a value within 
# the predicted value +/- predicted value *.3, it's
# around .87. So that's a bit better than
# what Krause gets but it's pretty close. 
# For pv +/- pv*.1 it's .38. So, that's a little
# worse than Krause but pretty close. 
# If we make the scaling factor smaller, 
# like .15, the latter estimate goes to .495
# which is over-optimistic. For pv*.3 it's
# .95 which is way too over-optimistic. 
# .2 seems ok. What about at the max
# value? Roughly 440k. For *.1 you get
# .38 which is pretty good. and for *.3
# you get .86. So, .2 seems like a good call. 
coef_var: .2

# Dictionary of foundation types 
# Triangular distributions for first-floor elevation conditioned
# on foundation type
# From this repo: https://github.com/HenryGeorgist/go-fathom/
# blob/master/compute/foundationheights.go
# from the wing et al. 2022 paper
# distributions (min, most likely, max) conditioned on foundation type
# Tri(0, .5, 1.5|Slab)
# Tri(0, 1.5, 4|Crawl)
# Tri(0, 1.5, 4|Basement)
# Tri(6, 9, 12|Pier)
# Tri (6, 9, 12|Pile)
# For this case study, only need 'S', 'C', and 'B'
ffe_dict:
  S: [0, .5, 1.5]
  C: [0, 1.5, 4]
  B: [0, 1.5, 4]

# Number of SOWs
sows: 10000

# Parameters for construction price indices (cpi)
# Bureau of labor statistics and census bureau
# More details in resources/ and in the reproduction
# instructions
bls_cpi: 1.41
cb_cpi: 1.85
# Parameters for elevation fixed costs
clara_elev_fixed: 20000
usace_elev_fixed: 70000
# Parameters for variable elevation costs
# which vary by level of heightening, 
# foundation type, and construction type
# These are stored in nested dicts
# The values correspond to 2 feet, 
# 4 feet, and 8 feet of elevation
# We will linearly interpolate 
# these points and limit from 3 feet
# to 10 feet
elev_cost_dict:
  wood: 
    basement: [29, 32, 37]
    slab: [80, 83, 88]
  masonry: 
    basement: [60, 63, 68]
    slab: [88, 91, 96]

# Project lifetime parameters
shape: 2.8
scale: 73.5