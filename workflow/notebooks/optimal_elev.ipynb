{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "\n",
    "import unsafe.files as unfile\n",
    "import unsafe.ddfs as unddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify FIPS, etc., \n",
    "fips_args = {\n",
    "    'FIPS': ['34007'], \n",
    "    'STATEFIPS': ['34'],\n",
    "    'STATEABBR': ['NJ'],\n",
    "    'NATION': ['US']\n",
    "}\n",
    "FIPS = fips_args['FIPS'][0]\n",
    "NATION = fips_args['NATION'][0]\n",
    "STATEABBR = fips_args['STATEABBR'][0]\n",
    "STATEFIPS = fips_args['STATEFIPS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the config file and set up key parameters\n",
    "ABS_DIR = Path().absolute().parents[1]\n",
    "\n",
    "CONFIG_FILEP = join(ABS_DIR, 'config', 'config.yaml')\n",
    "# Open the config file and load\n",
    "with open(CONFIG_FILEP) as f:\n",
    "    CONFIG = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "# First floor elevation dictionary\n",
    "FFE_DICT = CONFIG['ffe_dict']\n",
    "\n",
    "# Number of states of the world\n",
    "N_SOW = CONFIG['sows']\n",
    "\n",
    "# Return periods\n",
    "RET_PERS = CONFIG['RPs']\n",
    "\n",
    "# Hazard scenarios\n",
    "SCENARIOS = CONFIG['scenarios']\n",
    "\n",
    "# House lifetime parameters for discounting\n",
    "LT_SHAPE = CONFIG['shape']\n",
    "LT_SCALE = CONFIG['scale']\n",
    "\n",
    "# Elevation cost parameters\n",
    "CPI_LOW = CONFIG['bls_cpi']\n",
    "CPI_HIGH = CONFIG['cb_cpi']\n",
    "ELEV_FIX_LOW = CONFIG['clara_elev_fixed']\n",
    "ELEV_FIX_HIGH = CONFIG['usace_elev_fixed']\n",
    "ELEV_COST_DICT = CONFIG['elev_cost_dict']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick references to directories\n",
    "FR = join(ABS_DIR, \"data\", \"raw\")\n",
    "\n",
    "# And external - where our hazard data should be\n",
    "FE = join(FR, \"external\")\n",
    "\n",
    "# Set up interim and results directories as well\n",
    "# We already use \"FR\" for raw, we use \"FO\" \n",
    "# because you can also think of results\n",
    "# as output\n",
    "FI = join(ABS_DIR, \"data\", \"interim\")\n",
    "FO = join(ABS_DIR, \"data\", \"results\")\n",
    "\n",
    "# \"Raw\" data directories for exposure, vulnerability (vuln) and\n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, \"exp\")\n",
    "VULN_DIR_R = join(FR, \"vuln\")\n",
    "REF_DIR_R = join(FR, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_R = join(FE, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_R = join(FR, \"pol\")\n",
    "\n",
    "# Unzip directory \n",
    "UNZIP_DIR = join(FR, \"unzipped\")\n",
    "\n",
    "# We want to process unzipped data and move it\n",
    "# to the interim directory where we keep\n",
    "# processed data\n",
    "# Get the filepaths for unzipped data\n",
    "# We unzipped the depth grids (haz) and \n",
    "# ddfs (vuln) into the \"external\"/ subdirectory\n",
    "HAZ_DIR_UZ = join(UNZIP_DIR, \"external\", \"haz\")\n",
    "POL_DIR_UZ = join(UNZIP_DIR, \"pol\")\n",
    "REF_DIR_UZ = join(UNZIP_DIR, \"ref\")\n",
    "VULN_DIR_UZ = join(UNZIP_DIR, \"vuln\")\n",
    "DDF_DIR_UZ = join(UNZIP_DIR, \"external\", \"vuln\")\n",
    "\n",
    "# \"Interim\" data directories\n",
    "EXP_DIR_I = join(FI, \"exp\")\n",
    "VULN_DIR_I = join(FI, \"vuln\")\n",
    "REF_DIR_I = join(FI, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_I = join(FI, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_I = join(FI, \"pol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything that we do here is based on the ensemble values\n",
    "# That means we take the ffe variable in our ensemble df\n",
    "# and adjust it by the heightening amount, re-estimate losses\n",
    "# across all return periods, and re-estimate eal\n",
    "# In fact, since depths are \"fixed\" in our case study\n",
    "# we don't have to adjust the ffe variable, and can instead\n",
    "# adjust the depth_ffe_* columns\n",
    "\n",
    "# These are shared columns for subsetting\n",
    "# We need found_type because it is used in\n",
    "# elevation cost estimation\n",
    "# We need sqft because it's a key variable for\n",
    "# elevation cost estimation\n",
    "# We need bldgtype for elevation cost estimation, too\n",
    "sub_cols = ['fd_id', 'ffe',\n",
    "            'eal', 'val_s',\n",
    "            'sow_ind']\n",
    "# Some of these columns come from the nsi_sf.gpkg dataframe\n",
    "# Didn't save these with the ensemble because already in \n",
    "# nsi_sf.gpkg\n",
    "base_cols = ['fd_id', 'bldgtype', 'occtype', 'found_type', 'sqft']\n",
    "base_df = gpd.read_file(join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg'))\n",
    "base_df = base_df[base_cols]\n",
    "base_df['bld_types'] = base_df['occtype'].str.split('-').str[1]\n",
    "\n",
    "# We need to add depth_* columns (these are adjusted by first floor)\n",
    "depth_ffe_cols = ['depth_' + x for x in RET_PERS]\n",
    "sub_cols = sub_cols + depth_ffe_cols\n",
    "\n",
    "# Load the scenario data\n",
    "# There are only a few columns we need\n",
    "# Store them in a dict for each ensemble dataset\n",
    "ens_dfs = {}\n",
    "for scen in SCENARIOS:\n",
    "    ens_filep = join(FO, 'ensemble_' + scen + '.pqt')\n",
    "    ens_df = pd.read_parquet(ens_filep, columns=sub_cols)\n",
    "    # Merge with base_df to get the columns we need for\n",
    "    # cost estimation\n",
    "    ens_df = ens_df.merge(base_df, on='fd_id', how='left')\n",
    "    print('Load data: ' + scen)\n",
    "    ens_dfs[scen] = ens_df\n",
    "\n",
    "# We'll need DDFs for estimating benefits\n",
    "# Load DDFs\n",
    "naccs_ddfs = pd.read_parquet(join(VULN_DIR_I, 'physical', 'naccs_ddfs.pqt'))\n",
    "\n",
    "# Load helper dictionaries\n",
    "with open(join(VULN_DIR_I, 'physical', 'naccs.json'), 'r') as fp:\n",
    "    NACCS_MAX_DICT = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare discount rates and house lifetime\n",
    "\n",
    "# Download discount rate chains from external source\n",
    "# The rows correspond to house lifetime, indexed at 0\n",
    "# The columns correspond to states of the world, indexed at 0\n",
    "dr_chains = pd.read_csv(join(FE, 'dr_chains.csv'),\n",
    "                        header=None)\n",
    "\n",
    "# Following https://www.journals.uchicago.edu/doi/10.1086/718145\n",
    "# and https://doi.org/10.1162/rest_a_01109, replace values < 0 with 0\n",
    "# The economic argument is that descriptive discount rates will not\n",
    "# be less than 0 for long. Bauer and Rudebusch (the latter link)\n",
    "# have ~ 3 paragraphs addressing the mechanisms behind this\n",
    "# which I have paraphrased badly here. Some of the intuition is that\n",
    "# when nominal rates are low and inflation is high, households\n",
    "# can hold cash and reduce spending, bringing inflation down\n",
    "# and real rates back up. They offer a more complex, comprehsneive,\n",
    "# and convincing argument. I have had some conversations about\n",
    "# scrutinizing this assumption in a future paper. \n",
    "dr_chains[dr_chains < 0] = 0\n",
    "\n",
    "# Need to turn these into discount factors, following\n",
    "# Maggie's code\n",
    "# We need the rates as percentages, then we take the cumulative\n",
    "# sum of these such that the discount factor in year t\n",
    "# is the sum of all rates leading to that\n",
    "# Then we take e^- of that value\n",
    "dr_factors = np.exp(-(dr_chains/100).cumsum())\n",
    "\n",
    "# Generate house lifetime draws from the weibull distribution\n",
    "# following https://www.nature.com/articles/s41467-020-19188-9\n",
    "# Weibull with shape and scale parameters of 2.8 and 73.5\n",
    "# In numpy, you generate draws from a 1 parameter Weibull\n",
    "# using the shape parameter, and multiply these draws from\n",
    "# the scale parameter\n",
    "rng = np.random.default_rng()\n",
    "lifetime = np.round(rng.weibull(LT_SHAPE, N_SOW)*LT_SCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the config file take the inflation values, heightening values, \n",
    "# and fixed cost values. For heightening, we need to linearly interpolate\n",
    "# for our structure specific heightening cost estimates. For the others, \n",
    "# we need to generate N_SOW length realizations. We can pre-populate\n",
    "# a cost dataframe with this information and for each \n",
    "# foundation type, heightening combo, we will have\n",
    "# the SOW specific cost estimate to apply to the structure. \n",
    "# The costs are applied against the expected annual losses\n",
    "# to figure out the optimal heightening. We do not need to include\n",
    "# discount rates at this step since these are uniformly applied\n",
    "# in our case study since all the elevations are assumed to occur\n",
    "# at the same time. Time-based elevations that account for changing\n",
    "# cost estimates and discount rates is an extension of this work. \n",
    "\n",
    "# Do the interpolation on the elev costs\n",
    "# To get basement/bldgtype as multiindex from our dict\n",
    "elev_cost_df = pd.DataFrame.from_dict(ELEV_COST_DICT).stack().to_frame()\n",
    "# To break out the lists into columns\n",
    "# Column names as the foot value (as int) \n",
    "e_c_df = pd.DataFrame(elev_cost_df[0].values.tolist(),\n",
    "                      index=elev_cost_df.index,\n",
    "                      columns=[2, 4, 8]).reset_index()\n",
    "\n",
    "# Melt and rename to get ready for linear interpolation between feet\n",
    "e_c_df = e_c_df.melt(id_vars=['level_0', 'level_1'], value_vars=[2, 4, 8])\n",
    "e_c_df.columns = ['fnd_type', 'bldgtype', 'elev_ht', 'cost_sqft']\n",
    "\n",
    "# Loop through fnd_type, bldgtype groups\n",
    "# Add missing foot values and interpolate using\n",
    "# spline of order 1 to get values filled\n",
    "# past the 8 foot value and up to 10\n",
    "# Store each interpolated dataframe in a list and concat at the end\n",
    "elev_dfs = []\n",
    "for fnd_bldg, df_sub in e_c_df.groupby(['fnd_type', 'bldgtype']):\n",
    "    # keep track of foundation type and bldgtype\n",
    "    fnd = fnd_bldg[0]\n",
    "    bld = fnd_bldg[1]\n",
    "\n",
    "    # use elev ht as index and get a series of costs\n",
    "    elevs = df_sub.set_index('elev_ht')['cost_sqft']\n",
    "    # get the elevations from 2 to 3 feet that we are missing\n",
    "    missing_elevs = [x for x in np.arange(2, 11) if x not in elevs.index]\n",
    "    # combine elevs and missing elevs\n",
    "    elevs_f = pd.concat([elevs, pd.DataFrame(index=pd.Index(missing_elevs))])\n",
    "    # sort index and interpolate\n",
    "    elevs_f = elevs_f.sort_index().interpolate('spline', order=1).round(1)\n",
    "\n",
    "    # We consider elevation from 3 to 10 feet only\n",
    "    elevs_f = elevs_f.loc[3:10]\n",
    "\n",
    "    # Reset index and rename columns\n",
    "    elevs_f = elevs_f.reset_index()\n",
    "    elevs_f.columns = ['elev_ht', 'cost_sqft']\n",
    "    # Add back fnd_type and bldgtype\n",
    "    # using first character as capital letter\n",
    "    elevs_f['fnd_type'] = fnd[0].upper()\n",
    "    elevs_f['bldgtype'] = bld[0].upper()\n",
    "\n",
    "    elev_dfs.append(elevs_f)\n",
    "# Final cost per sqft dataframe\n",
    "elev_costs = pd.concat(elev_dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Sample N_SOW from uniform(CPI_LOW, CPI_HIGH)\n",
    "# Sample N_SOW from uniform (ELEV_FIX_LOW, ELEV_FIX_HIGH)\n",
    "rng = np.random.default_rng()\n",
    "construction_infl = rng.uniform(CPI_LOW, CPI_HIGH, N_SOW)\n",
    "fixed = rng.uniform(ELEV_FIX_LOW, ELEV_FIX_HIGH, N_SOW)\n",
    "\n",
    "# Get the cost dataframe for each sow_ind \n",
    "# Columns are sow_ind, bldgtype, heightening, cost\n",
    "# We need to multiply each cost_sqft value in elev_costs\n",
    "# by each element of construction_infl and make sure this is\n",
    "# indexed by the sow. Then, we need to add the fixed cost\n",
    "# corresponding to that sow\n",
    "\n",
    "# Repeat the elev_costs df so that each entry (ht, cst, types) has\n",
    "# N_SOW rows\n",
    "e_c_ens = elev_costs.loc[np.repeat(elev_costs.index, N_SOW)]\n",
    "e_c_ens = e_c_ens.reset_index(drop=True)\n",
    "# Then repeat the construction_infl and fixed series len(elev_costs)\n",
    "# times. Do this via tiling (i.e. repeat the whole array not \n",
    "# the elements) \n",
    "c_infl_full = np.tile(construction_infl, len(elev_costs))\n",
    "fixed_full = np.tile(fixed, len(elev_costs))\n",
    "\n",
    "# Now create new column in e_c_ens for\n",
    "# cost_sqft*c_infl_full and fixed_full\n",
    "e_c_ens['cost_sqft_unc'] = e_c_ens['cost_sqft']*c_infl_full\n",
    "e_c_ens['cost_fix_unc'] = fixed_full\n",
    "\n",
    "# Then get the sow_ind for the e_c_ens dataframe\n",
    "sow_ind = np.arange(len(e_c_ens))%N_SOW\n",
    "e_c_ens = pd.concat([e_c_ens, pd.Series(sow_ind, name='sow_ind')], axis=1)\n",
    "\n",
    "# Write out the elevation cost ensemble\n",
    "elev_ens_filep = join(EXP_DIR_I, FIPS, 'elev_ens.pqt')\n",
    "unfile.prepare_saving(elev_ens_filep)\n",
    "e_c_ens.to_parquet(elev_ens_filep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal elevation under uncetainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through possible heightenings of 3 through 10 feet, inclusive\n",
    "# For each of these, add that value to each column in depth_ffe_*\n",
    "# Then, go through the procedures from benchmark_ensemble\n",
    "# to calculate losses per return period and ultimately the eal\n",
    "# Then, we compare this eal to the non-elevated eal, which is \n",
    "# stored in the reference \"eal_col\"\n",
    "# We then take the subset of \n",
    "# fd_id, sow_ind, fnd_type, bldgtype, sqft, reduced_eal\n",
    "# and merge it with e_c_ens. (sow_ind, fnd_type, bldgtype)\n",
    "# Take reduced_eal/(sqft*cost_sqft_unc + cost_fix_unc) and store\n",
    "# it as bcr. Groupby on fd_id and calculate the mean bcr.\n",
    "# Store this as a series with name corresponding to the amount\n",
    "# of heightening and index corresponding to fd_id,\n",
    "# and store that series in a list\n",
    "# I think we also want to store the reduced_eal and\n",
    "# the costs for each sow. \n",
    "# After looping through all of these, we can concat our list\n",
    "# into a dataframe and figure out which column corresponds\n",
    "# to the highest bcr for each fd_id. I think we can do this\n",
    "# using df.idxmax(axis=\"columns\") if we concat on columns\n",
    "# Finally, we use the corresponding\n",
    "# heightening value to match up each fd_id to its\n",
    "# avoided loss, heightening, and expected costs. The BCR needs to \n",
    "# be adjusted in the allocate funding procedure later on by different\n",
    "# discount rate projections. I think we need to discount the \n",
    "# reduced_eal in each SOW AND divide each of those by costs again\n",
    "# to get the correct expected BCR. But you don't need to do \n",
    "# discounting to find the optimal elevation. \n",
    "\n",
    "for scen, ens_df in ens_dfs.items():\n",
    "    print('Scenario: ' + scen)\n",
    "    # Get fnd_type variable for ens_df that corresponds to B and S\n",
    "    # where crawl space (C) from found_type gets classified as B\n",
    "    # This is needed for a future step\n",
    "    ens_df['fnd_type'] = np.where(ens_df['found_type'] == 'S',\n",
    "                                  'S', \n",
    "                                  'B')\n",
    "\n",
    "    # We're going to make a lifetime_mask and dr_matrix \n",
    "    # to calculate present values of potential heightenings - the avoided\n",
    "    # losses as well as the residual risk\n",
    "    # Prepare lifetime mask and matrix for discount factors\n",
    "    \n",
    "    # discount factor matrix\n",
    "    dr_matrix = np.tile(dr_factors, (1, len(ens_df['fd_id'].unique())))\n",
    "    \n",
    "    # Use the lifetime series to create a mask\n",
    "    # Can adapt this code\n",
    "    # https://stackoverflow.com/questions/55190295/\n",
    "    # create-a-2-d-mask-from-a-1-d-numpy-array\n",
    "    # This code is complex, so I want to explain what is happening. You\n",
    "    # can also look at the stackoverflow link which provides helpful\n",
    "    # information. \n",
    "    # So, let's start from the inside out. The first command \n",
    "    # is np.less.outer(lifetime, np.arange(100))\n",
    "    # This takes the lifetime array, which is N_SOW in length\n",
    "    # and broadcasts that with outer into a N_SOW*100 shape 2d array\n",
    "    # 100 is the max lifetime we consider since discount rates are\n",
    "    # projected through 2100. We're comparing the values in lifetime\n",
    "    # to the values in the np.arange(100) array. When the lifetime\n",
    "    # value is less the element is assigned False because the value\n",
    "    # in the left series is greater. This creates\n",
    "    # a mask of True/False values which we need to match up to\n",
    "    # our 100x2390000 matrix of eal_avoid values. We are going to create\n",
    "    # a masked array later, and True means do not include\n",
    "    # the value in the calculation. We prepare for this first by\n",
    "    # transposing, then tiling along the columns the same number\n",
    "    # of times as we have structures in our sample.\n",
    "    lifetime_mask = np.tile(np.less.outer(lifetime, np.arange(100)).T,\n",
    "                            (1, len(ens_df['fd_id'].unique())))\n",
    "\n",
    "    # Write out the lifetime mask\n",
    "    lifetime_filename = 'lifetime_mask' + '_' + scen + '.npy'\n",
    "    lifetime_filep = join(EXP_DIR_I, FIPS, lifetime_filename)\n",
    "    with open(lifetime_filep, 'wb') as f:\n",
    "        np.save(f, lifetime_mask)\n",
    "\n",
    "    # List for series of mean bcr at each heightening\n",
    "    h_list = []\n",
    "    \n",
    "    for h in np.arange(3, 11):\n",
    "        # Adjust depth_ffe_* columns by h\n",
    "        # We substract h because these are \n",
    "        # depths relative to first floor\n",
    "        # and now the first floor is higher\n",
    "        depth_ffe_df = ens_df.loc[:,depth_ffe_cols] - h\n",
    "        # Remove 'depth_ffe_' part from the column\n",
    "        depth_ffe_df.columns = [x.split('_')[-1]\n",
    "                                for x in depth_ffe_df.columns]\n",
    "        \n",
    "        # We will store losses in dictionaries\n",
    "        # with return period keys\n",
    "        elev_losses = {}\n",
    "\n",
    "        # The reason I'm looping through ddf_type is because\n",
    "        # I think this way accommodates more flexibility in the\n",
    "        # future if there are other\n",
    "        # damage functions employed\n",
    "        for rp in RET_PERS:\n",
    "            elev_losses[rp] = unddf.est_naccs_loss(ens_df['bld_types'],\n",
    "                                                   depth_ffe_df[rp],\n",
    "                                                   naccs_ddfs,\n",
    "                                                   NACCS_MAX_DICT)\n",
    "        \n",
    "            print('Estimate Losses for Elevated Home, RP: ' + rp)\n",
    "        \n",
    "        # Then, we convert these to dataframes\n",
    "        loss_df = pd.DataFrame.from_dict(elev_losses)\n",
    "        \n",
    "        # For each relative damage column, \n",
    "        # scale by val_s\n",
    "        # loss_df and ens_df are index aligned, so this works\n",
    "        for col in loss_df.columns:\n",
    "            loss_df['loss_' + col] = loss_df[col]*ens_df['val_s']\n",
    "        \n",
    "        # Calculate eal\n",
    "        eal = unddf.get_eal(loss_df, RET_PERS)\n",
    "    \n",
    "        # Calculate avoided losses and add to ens_df\n",
    "        # Cannot be less than 0\n",
    "        eal_avoid_temp = ens_df['eal'] - eal\n",
    "        eal_avoid_temp[eal_avoid_temp < 0] = 0\n",
    "        ens_df['eal_avoid_' + str(h)] = eal_avoid_temp\n",
    "        \n",
    "        # Present value - avoided losses\n",
    "        eal_avoid = np.tile(ens_df['eal_avoid_' + str(h)], (100, 1))\n",
    "        # Apply the lifetime_mask to eal_avoid\n",
    "        eal_av_life = ma.masked_array(eal_avoid,\n",
    "                                        mask=lifetime_mask,\n",
    "                                        fill_value=0)\n",
    "        # present value \n",
    "        pv_avoided = (eal_av_life*dr_matrix).sum(axis=0)\n",
    "        # Add back into ens_df\n",
    "        ens_df['pv_avoid_' + str(h)] = pv_avoided.data\n",
    "        # Also get the relative avoided\n",
    "        ens_df['avoid_rel_eal_' + str(h)] = (ens_df['eal_avoid_' + str(h)]\n",
    "                                                /ens_df['val_s'])\n",
    "        \n",
    "        # Merge e_c_ens on subset of ens_df columns to figure out\n",
    "        # the elevation cost and get this into ens_df\n",
    "        ens_sub = ens_df[['fd_id', 'sow_ind', 'fnd_type',\n",
    "                          'bldgtype', 'sqft']].copy()\n",
    "        \n",
    "        # Also subset e_c_ens for the correct heightening\n",
    "        # Don't need cost_sqft for this either\n",
    "        e_c_ens_sub = e_c_ens[e_c_ens['elev_ht'] == h].drop(columns=['elev_ht',\n",
    "                                                                        'cost_sqft'])\n",
    "        \n",
    "        # Merge on sow, fnd_type, bldgtype\n",
    "        e_c_merge = ens_sub.merge(e_c_ens_sub,\n",
    "                                  on=['fnd_type', 'bldgtype', 'sow_ind'])\n",
    "        \n",
    "        # Get upfront costs\n",
    "        invsts = (e_c_merge['sqft']*e_c_merge['cost_sqft_unc']\n",
    "                    + e_c_merge['cost_fix_unc'])\n",
    "        ens_df['elev_invst_' + str(h)] = invsts\n",
    "    \n",
    "        # Present value - residual risk (our final_eal column)\n",
    "        eal_resid = np.tile(eal, (100, 1))\n",
    "        # Apply the lifetime_mask to eal_resid\n",
    "        eal_r_life = ma.masked_array(eal_resid,\n",
    "                                     mask=lifetime_mask,\n",
    "                                     fill_value=0)\n",
    "        # present value \n",
    "        pv_resid = (eal_r_life*dr_matrix).sum(axis=0)\n",
    "        # Add back into ens_df\n",
    "        ens_df['pv_resid_' + str(h)] = pv_resid.data\n",
    "        # Also get the relative resid\n",
    "        ens_df['resid_rel_eal_' + str(h)] = eal/ens_df['val_s']\n",
    "    \n",
    "        # Get costs\n",
    "        # Add present value of residual risk to upfront cost\n",
    "        ens_df['elev_cost_' + str(h)] = (ens_df['pv_resid_' + str(h)]\n",
    "                                            + ens_df['elev_invst_' + str(h)])\n",
    "        \n",
    "        # Now we have the avoided loss and elev cost for this level of\n",
    "        # heightening stored in ens_df\n",
    "        # It also helps to do some side calculations to save some time\n",
    "        # later in obtaining the optimal level of heightening\n",
    "        # Get the ratio of eal_avoid_str(h) to elev_cost_str(h)\n",
    "        # Groupby on fd_id and take the mean\n",
    "        # Store this as a series with name corresponding to the amount\n",
    "        # of heightening and index corresponding to fd_id,\n",
    "        # and store that series in a list\n",
    "        ens_df['npv_' + str(h)] = (ens_df['pv_avoid_' + str(h)]\n",
    "                                    - ens_df['elev_cost_' + str(h)])\n",
    "        \n",
    "        npvs = ens_df.groupby(['fd_id'])['npv_' + str(h)].mean()\n",
    "        h_list.append(npvs)\n",
    "    \n",
    "        print('Calculations done for heightening by ' + str(h) + ' feet\\n')\n",
    "    \n",
    "    # Get the dataframe of mean bcr across SOWs for each structure\n",
    "    # for each heightening. Find the heightening for each structure\n",
    "    # that leads to the max mean bcr, and write this out to a file. \n",
    "    # When we do the full bcr estimation later, we will loop through\n",
    "    # each value in this series, subset the ens_df based on\n",
    "    # fd_id with that bcr_part_h as their max mean bcr, and then\n",
    "    # do the full discounting and recalculation of BCR. You need\n",
    "    # to use the discount rate chain that corresponds to each SOW\n",
    "    # to discount the avoided losses correctly\n",
    "    # and you need to divide that by the costs in that SOW\n",
    "    # THEN you can calculate our objectives like net benefits and\n",
    "    # check conditions like BCR > 1. \n",
    "    opt_elev = pd.concat(h_list, axis=1)\n",
    "    opt_elev['opt_elev'] = opt_elev.idxmax('columns')\n",
    "    npv_out_filename = 'opt_height_' + scen + '.pqt'\n",
    "    npv_out_filep = join(EXP_DIR_I, FIPS, npv_out_filename)\n",
    "    unfile.prepare_saving(npv_out_filep)\n",
    "    opt_elev.to_parquet(npv_out_filep) \n",
    "\n",
    "    # Write out ens_df columns related to optimal elevation\n",
    "    # eal_avoid_h and elev_cost_h\n",
    "    # Only need to do this for the heightening that corresponds\n",
    "    # to the optimal level\n",
    "    # Subset ens_df based on the information in opt_elev\n",
    "    # Do this by looping through the values in opt_elev, getting the list\n",
    "    # of fd_id that correspond to this, and then storing the ens_df\n",
    "    # rows & columns (eal_avoid_h and elev_cost_h) in a list along\n",
    "    # with the heightening amount\n",
    "    # You will end up concatenating a dataframe that is\n",
    "    # sow_ind, fd_id, eal_avoid_opt, elev_cost_opt, elev_h\n",
    "    \n",
    "    elev_df_l = []\n",
    "    for elev_h in opt_elev['opt_elev'].unique():\n",
    "        # Subset of fd_id that have this optimal heightening\n",
    "        struct_sub = opt_elev[opt_elev['opt_elev'] == elev_h].index\n",
    "        # elev value\n",
    "        h = elev_h.split('_')[-1]\n",
    "        # Corresponding columns\n",
    "        ens_col_sub = ['pv_avoid_' + str(h), 'elev_cost_' + str(h),\n",
    "                        'elev_invst_' + str(h), 'pv_resid_' + str(h),\n",
    "                        'fd_id', 'sow_ind', \n",
    "                        'avoid_rel_eal_' + str(h), 'resid_rel_eal_' + str(h),\n",
    "                        'eal']\n",
    "        # Corresponding rows and columns\n",
    "        ens_sub = ens_df.loc[ens_df['fd_id'].isin(struct_sub),\n",
    "                                ens_col_sub]\n",
    "        # Rename columns\n",
    "        ens_sub.columns = ['pv_avoid', 'pv_cost', \n",
    "                            'elev_invst', 'pv_resid',\n",
    "                            'fd_id', 'sow_ind', \n",
    "                            'avoid_rel_eal', 'resid_rel_eal', 'base_eal']\n",
    "        # Add the heightening amount back in\n",
    "        ens_sub['opt_elev'] = h\n",
    "        \n",
    "        elev_df_l.append(ens_sub)\n",
    "        print('Processed rows with optimal elevation height of ' + str(h))\n",
    "    \n",
    "    elev_df_f = pd.concat(elev_df_l, axis=0).sort_index()\n",
    "    \n",
    "    # Should also calculate present value of the 'base' eal\n",
    "    # and write out the lifetime data that was generated\n",
    "    eal_base = np.tile(elev_df_f['base_eal'], (100, 1))\n",
    "    # Apply the lifetime_mask to eal_base\n",
    "    eal_life = ma.masked_array(eal_base,\n",
    "                                mask=lifetime_mask,\n",
    "                                fill_value=0)\n",
    "    # present value \n",
    "    pv_base = (eal_life*dr_matrix).sum(axis=0)\n",
    "    # Add back into ens_df\n",
    "    elev_df_f['pv_base'] = pv_base.data\n",
    "\n",
    "    # Write file in FIPS specific exp/ directory\n",
    "    opt_elev_filename = 'ens_opt_elev_' + scen + '.pqt'\n",
    "    opt_elev_filep = join(EXP_DIR_I, FIPS, opt_elev_filename)\n",
    "    elev_df_f.to_parquet(opt_elev_filep)\n",
    "    print('Wrote file: ' + opt_elev_filename + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gc_elev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
