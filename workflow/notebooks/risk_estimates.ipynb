{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "from shapely.geometry import shape\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "import unsafe.download as undown\n",
    "import unsafe.files as unfile\n",
    "import unsafe.unzip as ununzip\n",
    "import unsafe.exp as unexp\n",
    "import unsafe.ddfs as unddf\n",
    "import unsafe.ensemble as unens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify FIPS, etc., \n",
    "fips_args = {\n",
    "    'FIPS': ['34007'], \n",
    "    'STATEFIPS': ['34'],\n",
    "    'STATEABBR': ['NJ'],\n",
    "    'NATION': ['US']\n",
    "}\n",
    "FIPS = fips_args['FIPS'][0]\n",
    "NATION = fips_args['NATION'][0]\n",
    "STATEABBR = fips_args['STATEABBR'][0]\n",
    "STATEFIPS = fips_args['STATEFIPS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the config file and set up key parameters\n",
    "ABS_DIR = Path().absolute().parents[1]\n",
    "\n",
    "CONFIG_FILEP = join(ABS_DIR, 'config', 'config.yaml')\n",
    "# Open the config file and load\n",
    "with open(CONFIG_FILEP) as f:\n",
    "    CONFIG = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "# Wildcards for urls\n",
    "URL_WILDCARDS = CONFIG['url_wildcards']\n",
    "\n",
    "# Get the file extensions for api endpoints\n",
    "API_EXT = CONFIG['api_ext']\n",
    "\n",
    "# Get the CRS constants\n",
    "NSI_CRS = CONFIG['nsi_crs']\n",
    "\n",
    "# Dictionary of ref_names\n",
    "REF_NAMES_DICT = CONFIG['ref_names']\n",
    "\n",
    "# Dictionary of ref_id_names\n",
    "REF_ID_NAMES_DICT = CONFIG['ref_id_names']\n",
    "\n",
    "# Coefficient of variation\n",
    "# for structure values\n",
    "COEF_VARIATION = CONFIG['coef_var']\n",
    "\n",
    "# First floor elevation dictionary\n",
    "FFE_DICT = CONFIG['ffe_dict']\n",
    "\n",
    "# Use gloucester city for clipping\n",
    "# store its CRS\n",
    "CLIP_CRS = CONFIG['clip_crs']\n",
    "\n",
    "# Number of states of the world\n",
    "N_SOW = CONFIG['sows']\n",
    "\n",
    "# Get hazard model variables\n",
    "# We have a hazard subdirectory of the scenario-based runs\n",
    "HAZ_DIR_SUB = CONFIG['haz_dir_sub']\n",
    "# Get Return Period list\n",
    "RET_PERS = CONFIG['RPs']\n",
    "HAZ_FILEN = CONFIG['haz_filename']\n",
    "# Get CRS for depth grids\n",
    "HAZ_CRS = CONFIG['haz_crs']\n",
    "\n",
    "# Hazard scenarios\n",
    "SCENARIOS = CONFIG['scenarios']\n",
    "\n",
    "# Get the files we need downloaded\n",
    "DOWNLOAD = pd.json_normalize(CONFIG['download'], sep='_').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick references to directories\n",
    "FR = join(ABS_DIR, \"data\", \"raw\")\n",
    "\n",
    "# And external - where our hazard data should be\n",
    "FE = join(FR, \"external\")\n",
    "\n",
    "# Set up interim and results directories as well\n",
    "# We already use \"FR\" for raw, we use \"FO\" \n",
    "# because you can also think of results\n",
    "# as output\n",
    "FI = join(ABS_DIR, \"data\", \"interim\")\n",
    "FO = join(ABS_DIR, \"data\", \"results\")\n",
    "\n",
    "# \"Raw\" data directories for exposure, vulnerability (vuln) and\n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, \"exp\")\n",
    "VULN_DIR_R = join(FR, \"vuln\")\n",
    "REF_DIR_R = join(FR, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_R = join(FE, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_R = join(FR, \"pol\")\n",
    "\n",
    "# Unzip directory \n",
    "UNZIP_DIR = join(FR, \"unzipped\")\n",
    "\n",
    "# We want to process unzipped data and move it\n",
    "# to the interim directory where we keep\n",
    "# processed data\n",
    "# Get the filepaths for unzipped data\n",
    "# We unzipped the depth grids (haz) and \n",
    "# ddfs (vuln) into the \"external\"/ subdirectory\n",
    "HAZ_DIR_UZ = join(UNZIP_DIR, \"external\", \"haz\")\n",
    "POL_DIR_UZ = join(UNZIP_DIR, \"pol\")\n",
    "REF_DIR_UZ = join(UNZIP_DIR, \"ref\")\n",
    "VULN_DIR_UZ = join(UNZIP_DIR, \"vuln\")\n",
    "DDF_DIR_UZ = join(UNZIP_DIR, \"external\", \"vuln\")\n",
    "\n",
    "# \"Interim\" data directories\n",
    "EXP_DIR_I = join(FI, \"exp\")\n",
    "VULN_DIR_I = join(FI, \"vuln\")\n",
    "REF_DIR_I = join(FI, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_I = join(FI, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_I = join(FI, \"pol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcard_dict = {x: fips_args[x[1:-1]][0] for x in URL_WILDCARDS}\n",
    "undown.download_raw(DOWNLOAD, wcard_dict,\n",
    "                    FR, API_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ununzip.unzip_raw(FR, UNZIP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data we downloaded from the county's REST API server\n",
    "clip_filep = join(REF_DIR_R, FIPS, 'clip.json')\n",
    "with open(clip_filep) as f:\n",
    "    clip_data = json.load(f)\n",
    "\n",
    "# Use pandas to get the data in a form that is easier\n",
    "# to turn into a geodataframe for clipping\n",
    "clip_df = pd.json_normalize(clip_data['features'])\n",
    "# We want to make a polygon out of the geometry coordinates\n",
    "# We can access that from the original json object\n",
    "clip_geo = [shape(i['geometry']) for i in clip_data['features']]\n",
    "# We can create a geodataframe of clip_df by adding clip_geo\n",
    "# as its geometry column\n",
    "clip_gdf = gpd.GeoDataFrame(clip_df,\n",
    "                            crs=CLIP_CRS,\n",
    "                            geometry=clip_geo)\n",
    "\n",
    "# We can clean up the gdf by removing the\n",
    "# type, id, geometry.type and geometry.coordinates columns\n",
    "drop_col = ['type', 'id', 'geometry.type', 'geometry.coordinates']\n",
    "clip_gdf = clip_gdf.drop(columns=drop_col)\n",
    "\n",
    "# Write the file out to interim\n",
    "clip_out_filep = join(FI, 'ref', FIPS, 'clip.gpkg')\n",
    "unfile.prepare_saving(clip_out_filep)\n",
    "clip_gdf.to_file(clip_out_filep,\n",
    "                 driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsi_gdf = unexp.get_nsi_geo(FIPS, NSI_CRS, EXP_DIR_R)\n",
    "\n",
    "# Set the values that we pass into the get_struct_subset\n",
    "# function. In this case, occtype==RES1 and num_story <= 2\n",
    "occtype_list=['RES1-1SNB', 'RES1-2SNB', 'RES1-1SWB', 'RES1-2SWB']\n",
    "sub_string = 'occtype.isin(@occtype_list) and num_story <= 2'\n",
    "nsi_filt = unexp.get_struct_subset(nsi_gdf,\n",
    "                                   filter=sub_string,\n",
    "                                   occtype_list=occtype_list)\n",
    "\n",
    "# For this case study, let us save some memory and just\n",
    "# write out the single family houses \n",
    "EXP_OUT_FILEP = join(EXP_DIR_I, FIPS, 'nsi_sf.gpkg')\n",
    "unfile.prepare_saving(EXP_OUT_FILEP)\n",
    "\n",
    "# Clip to our clip boundary\n",
    "# They are in the same CRS\n",
    "nsi_clip_out = gpd.clip(nsi_filt, clip_gdf)\n",
    "\n",
    "# Limit to sqft <= 99th percentile\n",
    "# Arbitrary cutoff. The max value from the steps above\n",
    "# is 400858 which is way too large\n",
    "# There are other large values that are dropped with this\n",
    "# arbitrary cutoff\n",
    "# For GC case study, this value is 2696.41999\n",
    "sqft_clip = nsi_clip_out['sqft'].quantile(.99)\n",
    "nsi_clip_out[nsi_clip_out['sqft'] <= sqft_clip].to_file(EXP_OUT_FILEP,\n",
    "                                                        driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to clip reference data to \n",
    "# the GC clip file from earlier\n",
    "unexp.clip_ref_files(clip_gdf, FIPS,\n",
    "                     REF_DIR_UZ, REF_DIR_I, REF_NAMES_DICT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unddf.process_naccs(DDF_DIR_UZ, VULN_DIR_I)\n",
    "# Belownot used in this case study, but\n",
    "# because of a pending issue with UNSAFE, we need to process these\n",
    "unddf.process_hazus(DDF_DIR_UZ, VULN_DIR_I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the social vulnerability data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process national social vulnerability data\n",
    "sovi_list = ['cejst', 'svi']\n",
    "unexp.process_national_sovi(sovi_list, FIPS,\n",
    "                            VULN_DIR_R, REF_DIR_I, VULN_DIR_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process lmi & nj ovb at block group\n",
    "# (not yet in unsafe)\n",
    "bg_filep = join(REF_DIR_I, FIPS, 'bg.gpkg')\n",
    "bg_geo = gpd.read_file(bg_filep)\n",
    "\n",
    "# Process lmi\n",
    "# Read data\n",
    "lmi_filename = 'ACS_2015_lowmod_blockgroup_all.xlsx'\n",
    "lmi_filep = join(VULN_DIR_R, 'social', NATION, lmi_filename)\n",
    "lmi = pd.read_excel(lmi_filep, engine='openpyxl')\n",
    "# Get GEOID for merge (last 12 characters is the bg id)\n",
    "lmi['GEOID'] = lmi['GEOID'].str[-12:]\n",
    "\n",
    "# Retain GEOID and Lowmod_pct\n",
    "keep_cols = ['GEOID', 'Lowmod_pct']\n",
    "lmi_f = bg_geo[['GEOID', 'geometry']].merge(lmi[keep_cols],\n",
    "                                            on='GEOID',\n",
    "                                            how='inner')\n",
    "\n",
    "# Write file\n",
    "lmi_out_filep = join(VULN_DIR_I, 'social', FIPS, 'lmi.gpkg')\n",
    "lmi_f.to_file(lmi_out_filep, driver='GPKG')\n",
    "\n",
    "\n",
    "# Process NJ overburdened\n",
    "# Read data\n",
    "ovb_filep = join(VULN_DIR_UZ, 'social', STATEABBR,\n",
    "                 'Govt_census_group_2022_EJ.gdb')\n",
    "ovb = gpd.read_file(ovb_filep)\n",
    "\n",
    "# Rename some columns\n",
    "ovb = ovb.rename(columns={'OVERBURDENED_COMMUNITY_CRITERI': 'ovb_crit'})\n",
    "\n",
    "# Keep a subset of columns\n",
    "ovb_f = ovb[['GEOID', 'ovb_crit', 'geometry']]\n",
    "\n",
    "# The data already is limited to overburdened categories\n",
    "\n",
    "# Subset to our study area\n",
    "ovb_reproj = ovb_f.to_crs(clip_gdf.crs)\n",
    "ovb_clipped = gpd.clip(ovb_reproj, clip_gdf)\n",
    "\n",
    "# Write file\n",
    "ovb_out_filep = join(VULN_DIR_I, 'social', FIPS, 'ovb.gpkg')\n",
    "ovb_clipped.to_file(ovb_out_filep, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link the social vulnerability data to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the community boundary data\n",
    "# Get links to the single family home data\n",
    "# Store in single dataframe\n",
    "# Write out\n",
    "\n",
    "sovi_dir = join(VULN_DIR_I, 'social', FIPS)\n",
    "filenames = ['lmi', 'sovi', 'ovb', 'cejst']\n",
    "\n",
    "sovi_df_list = []\n",
    "# Let's add a list of just fd_id\n",
    "# This makes sure every property is\n",
    "# linked to the social vulnerability categories\n",
    "sovi_df_list.append(nsi_clip_out[['fd_id']].set_index('fd_id'))\n",
    "\n",
    "for fn in filenames:\n",
    "    # Read in each gpkg\n",
    "    fp = join(sovi_dir, fn + '.gpkg')\n",
    "    sovi_geo = gpd.read_file(fp)\n",
    "\n",
    "    # Subset sovi_geo based on thresholds\n",
    "    # For cejst and ovb this is already done\n",
    "    # For lmi and ovb need to do the filter as follows\n",
    "    if fn == 'lmi':\n",
    "        # See https://www.hudoig.gov/reports-publications/\n",
    "        # report/cdbg-dr-program-generally-\n",
    "        # met-low-and-moderate-income-requirements\n",
    "        # The statutory threshold is 50%, so retain those\n",
    "        sovi_sub = sovi_geo[sovi_geo['Lowmod_pct'] > .5]\n",
    "    elif fn == 'sovi':\n",
    "        # Subset to threshhold for FMA (from 2022 NOFO)\n",
    "        sovi_sub = sovi_geo[sovi_geo['sovi'] > .6]\n",
    "    elif fn == 'ovb':\n",
    "        sovi_sub = sovi_geo[sovi_geo['ovb_crit'] != 'Adjacent']\n",
    "    else:\n",
    "        sovi_sub = sovi_geo\n",
    "\n",
    "    # Only need the geometry for sovi_sub\n",
    "    sovi_sub = sovi_sub[['geometry']]\n",
    "    \n",
    "    # Limit the NSI to our fd_id and geometry column\n",
    "    keep_col_nsi = ['fd_id', 'geometry']\n",
    "    nsi_sub = nsi_clip_out[keep_col_nsi]\n",
    "\n",
    "    # Reproj nsi_sub to the reference crs\n",
    "    nsi_reproj = nsi_sub.to_crs(sovi_geo.crs)\n",
    "\n",
    "    # Do a spatial join\n",
    "    nsi_sovi = gpd.sjoin(nsi_reproj, sovi_sub, predicate='within')\n",
    "\n",
    "    # Add indicator column\n",
    "    nsi_sovi[fn] = True\n",
    "\n",
    "    # Append this to our sovi_df_list\n",
    "    sovi_df_list.append(nsi_sovi[['fd_id', fn]].set_index('fd_id'))\n",
    "\n",
    "    # Helpful message\n",
    "    print('Linked vulnerability to NSI: ' + fn)\n",
    "\n",
    "sovi_df_f = pd.concat(sovi_df_list, axis=1).fillna(False)\n",
    "sovi_out_filepath = join(sovi_dir, 'c_indicators.pqt')\n",
    "sovi_df_f.to_parquet(sovi_out_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need NFHL for the ensemble and visualizations\n",
    "unexp.process_nfhl(FIPS,\n",
    "                   POL_DIR_UZ,\n",
    "                   POL_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link food zones and references to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link flood zones\n",
    "nfhl_filep = join(POL_DIR_I, FIPS, 'fld_zones.gpkg')\n",
    "nfhl = gpd.read_file(nfhl_filep)\n",
    "keep_cols = ['fld_zone', 'fld_ar_id', 'static_bfe']\n",
    "unexp.get_spatial_var(nsi_clip_out,\n",
    "                      nfhl,\n",
    "                      'fz',\n",
    "                      FIPS,\n",
    "                      EXP_DIR_I,\n",
    "                      keep_cols)\n",
    "\n",
    "# Link references\n",
    "unexp.get_ref_ids(nsi_clip_out, FIPS,\n",
    "                  REF_ID_NAMES_DICT, REF_DIR_I, EXP_DIR_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The files unzipped into haz_dir_uz_{scen}/haz_dir_sub\n",
    "# We want to organize it a bit better as haz_dir_i/haz_filen\n",
    "# For each depth grid\n",
    "# move haz_dir_uz_scen/haz_dir_sub/haz_filen (without the _scen)\n",
    "# Use Path(from_file).rename(to_file) to organize\n",
    "# the depth grids better\n",
    "\n",
    "# Loop through haz_dir_uz_{scen} files\n",
    "for scen in SCENARIOS:\n",
    "    haz_from_dir = join(HAZ_DIR_UZ,\n",
    "                        HAZ_DIR_SUB + \"_\" + scen,\n",
    "                        HAZ_DIR_SUB)\n",
    "\n",
    "    pathlist = Path(haz_from_dir).glob('**/*.asc')\n",
    "    for path in pathlist:\n",
    "        haz_from_file = str(path)   \n",
    "        haz_to_file_sub = haz_from_file + \"_\" + scen\n",
    "        ret_per = haz_from_file.split(\".\")[0].split(\"_\")[-1]\n",
    "        haz_to_file = join(HAZ_DIR_I, HAZ_DIR_SUB,\n",
    "                        ret_per + \"_\" + scen + \".asc\")\n",
    "\n",
    "        unfile.prepare_saving(haz_to_file)\n",
    "        Path(haz_from_file).rename(haz_to_file)\n",
    "\n",
    "    print(\"Moved and renamed \" + scen + \" depth grids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the inundation grids and write out the\n",
    "# fd_id/depths dataframe\n",
    "depth_df = unexp.get_inundations(nsi_clip_out,\n",
    "                                 HAZ_CRS, RET_PERS,\n",
    "                                 join(HAZ_DIR_I, HAZ_DIR_SUB),\n",
    "                                 HAZ_FILEN,\n",
    "                                 scens=SCENARIOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out dataframe that links fd_id to depths\n",
    "# with columns corresponding to ret_per (i.e. 500, 100, 50, 10)\n",
    "# in our case study\n",
    "nsi_depths_out = join(EXP_DIR_I, FIPS, 'nsi_depths.pqt')\n",
    "unfile.prepare_saving(nsi_depths_out)\n",
    "depth_df.reset_index().to_parquet(nsi_depths_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe conducive for loss estimation\n",
    "# This procedure is separate fr mmpreparing data for the ensemble\n",
    "# so will just take the county code to load in and merge\n",
    "# all the relevant data\n",
    "base_df = unens.get_base_df(FIPS, EXP_DIR_I)\n",
    "\n",
    "# Get the ensemble\n",
    "# To make this run faster, we should subset\n",
    "# base_df on properties at risk in each scenario\n",
    "for scen in SCENARIOS:\n",
    "    print('Scenario: ' + scen)\n",
    "    # Subset to depths greater than 0\n",
    "    base_df_temp = base_df[base_df['depth_' + scen + '_500'] > 0]\n",
    "    # Want to remove the depth columns for other scenarios\n",
    "    d_cols_all = set(['depth_' + s  + '_' + rp for rp in RET_PERS for s in SCENARIOS])\n",
    "    d_cols_scen = set(['depth_' + scen  + '_' + rp for rp in RET_PERS])\n",
    "    d_cols_drop = list(d_cols_all - d_cols_scen)\n",
    "    base_df_temp = base_df_temp.drop(columns=d_cols_drop)\n",
    "\n",
    "    # If depths in other return periods are 0, make them nan\n",
    "    base_df_temp.loc[:,list(d_cols_scen)] = base_df_temp.loc[:,list(d_cols_scen)].replace({0:np.nan})\n",
    "\n",
    "    # Want to remove scen reference from columns\n",
    "    base_df_temp.columns = [x.replace('_' + scen, '') for x in base_df_temp.columns]\n",
    "\n",
    "    ens_df_losses  = unens.generate_ensemble(nsi_clip_out,\n",
    "                                             base_df_temp,\n",
    "                                             ['naccs'],\n",
    "                                             ['ffe', 'val_struct'],\n",
    "                                             N_SOW,\n",
    "                                             FFE_DICT,\n",
    "                                             COEF_VARIATION,\n",
    "                                             VULN_DIR_I)\n",
    "    \n",
    "    col_sub = [x for x in ens_df_losses.columns if 'loss' in x]\n",
    "    loss_sub = ens_df_losses[col_sub]\n",
    "    # Just get return period reference\n",
    "    ret_per_ints = [int(x.split('_')[-1]) for x in loss_sub.columns]\n",
    "    loss_sub.columns = ['loss_' + str(x) for x in ret_per_ints]\n",
    "    rp_list = sorted(ret_per_ints)\n",
    "    eals = unddf.get_eal(loss_sub, rp_list)\n",
    "\n",
    "    ens_df_out = pd.concat([ens_df_losses, pd.Series(eals, name='eal')],\n",
    "                           axis=1)\n",
    "    \n",
    "    out_file = 'ensemble_' + scen + '.pqt'\n",
    "    ens_out_filep = join(FO, out_file)\n",
    "    unfile.prepare_saving(ens_out_filep)\n",
    "    ens_df_out.to_parquet(join(FO, out_file))\n",
    "\n",
    "    print('Wrote losses & eal for ' + scen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flrisk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
