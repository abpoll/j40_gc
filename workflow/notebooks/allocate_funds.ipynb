{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "\n",
    "import unsafe.files as unfile\n",
    "import unsafe.ddfs as unddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify FIPS, etc., \n",
    "fips_args = {\n",
    "    'FIPS': ['34007'], \n",
    "    'STATEFIPS': ['34'],\n",
    "    'STATEABBR': ['NJ'],\n",
    "    'NATION': ['US']\n",
    "}\n",
    "FIPS = fips_args['FIPS'][0]\n",
    "NATION = fips_args['NATION'][0]\n",
    "STATEABBR = fips_args['STATEABBR'][0]\n",
    "STATEFIPS = fips_args['STATEFIPS'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the config file and set up key parameters\n",
    "ABS_DIR = Path().absolute().parents[1]\n",
    "\n",
    "CONFIG_FILEP = join(ABS_DIR, 'config', 'config.yaml')\n",
    "# Open the config file and load\n",
    "with open(CONFIG_FILEP) as f:\n",
    "    CONFIG = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "# Number of states of the world\n",
    "N_SOW = CONFIG['sows']\n",
    "\n",
    "# Hazard scenarios\n",
    "SCENARIOS = CONFIG['scenarios']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick references to directories\n",
    "FR = join(ABS_DIR, \"data\", \"raw\")\n",
    "\n",
    "# And external - where our hazard data should be\n",
    "FE = join(FR, \"external\")\n",
    "\n",
    "# Set up interim and results directories as well\n",
    "# We already use \"FR\" for raw, we use \"FO\" \n",
    "# because you can also think of results\n",
    "# as output\n",
    "FI = join(ABS_DIR, \"data\", \"interim\")\n",
    "FO = join(ABS_DIR, \"data\", \"results\")\n",
    "\n",
    "# \"Raw\" data directories for exposure, vulnerability (vuln) and\n",
    "# administrative reference files\n",
    "EXP_DIR_R = join(FR, \"exp\")\n",
    "VULN_DIR_R = join(FR, \"vuln\")\n",
    "REF_DIR_R = join(FR, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_R = join(FE, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_R = join(FR, \"pol\")\n",
    "\n",
    "# Unzip directory \n",
    "UNZIP_DIR = join(FR, \"unzipped\")\n",
    "\n",
    "# We want to process unzipped data and move it\n",
    "# to the interim directory where we keep\n",
    "# processed data\n",
    "# Get the filepaths for unzipped data\n",
    "# We unzipped the depth grids (haz) and \n",
    "# ddfs (vuln) into the \"external\"/ subdirectory\n",
    "HAZ_DIR_UZ = join(UNZIP_DIR, \"external\", \"haz\")\n",
    "POL_DIR_UZ = join(UNZIP_DIR, \"pol\")\n",
    "REF_DIR_UZ = join(UNZIP_DIR, \"ref\")\n",
    "VULN_DIR_UZ = join(UNZIP_DIR, \"vuln\")\n",
    "DDF_DIR_UZ = join(UNZIP_DIR, \"external\", \"vuln\")\n",
    "\n",
    "# \"Interim\" data directories\n",
    "EXP_DIR_I = join(FI, \"exp\")\n",
    "VULN_DIR_I = join(FI, \"vuln\")\n",
    "REF_DIR_I = join(FI, \"ref\")\n",
    "# Haz is for depth grids\n",
    "HAZ_DIR_I = join(FI, \"haz\")\n",
    "# Pol is for NFHL\n",
    "POL_DIR_I = join(FI, \"pol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_filepath = join(ABS_DIR, \"data\", \"check\", \"check_structures_final.csv\")\n",
    "check_df = pd.read_csv(check_filepath)\n",
    "drop_ids = check_df[check_df['is_house'] != True]['fd_id']\n",
    "\n",
    "# Load the ensemble data, along with the optimal\n",
    "# elevation results\n",
    "sort_dfs = {}\n",
    "ens_dfs = {}\n",
    "\n",
    "for scen in SCENARIOS:\n",
    "    ens_filep = join(FO, 'ensemble_' + scen + '.pqt')\n",
    "    ens_df = pd.read_parquet(ens_filep)\n",
    "    print('Load data: ' + scen)\n",
    "\n",
    "    # We keep observations that we are most confident are homes\n",
    "    ens_df = ens_df[~ens_df['fd_id'].isin(drop_ids)]\n",
    "\n",
    "    opt_elev_filename = 'ens_opt_elev_' + scen + '.pqt'\n",
    "    opt_elev_df = pd.read_parquet(join(EXP_DIR_I, FIPS, opt_elev_filename))\n",
    "    print('Load opt elev')\n",
    "    \n",
    "    # Merge on fd_id and sow_ind to get eal_avoid, elev_cost, and opt_elev\n",
    "    # into the ensemble\n",
    "    ens_df_m = ens_df.merge(opt_elev_df,\n",
    "                            on=['fd_id', 'sow_ind'],\n",
    "                            suffixes=['','_opt'])\n",
    "\n",
    "    # Add metrics for objectives that we don't have yet\n",
    "    ens_df_m['rel_eal'] = ens_df_m['base_eal']/ens_df_m['val_s']\n",
    "    ens_df_m['npv_opt'] = ens_df_m['pv_avoid'] - ens_df_m['elev_invst']\n",
    "\n",
    "    ens_dfs[scen] = ens_df_m\n",
    "\n",
    "    print('Store ensemble df\\n')\n",
    "    \n",
    "    # We need to group by on fd_id and aggregate on our sorting columns\n",
    "    sub_cols = ['pv_resid', 'npv_opt', 'fd_id', 'elev_invst',\n",
    "                'avoid_rel_eal', 'rel_eal', 'val_s']\n",
    "    sort_df = ens_df_m.groupby('fd_id')[sub_cols].mean()\n",
    "    \n",
    "    sort_dfs[scen] = sort_df\n",
    "\n",
    "    print('Store df for sorting\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to load in the links between structures and the\n",
    "# social vulnerability data for sorting rules\n",
    "sovi_filepath = join(VULN_DIR_I, 'social', FIPS, 'c_indicators.pqt')\n",
    "sovi_df = pd.read_parquet(sovi_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocate funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the risk-burden inequality\n",
    "\n",
    "def risk_val_gini(risk, val):\n",
    "    risk = np.asarray(risk)\n",
    "    val = np.asarray(val)\n",
    "    # Sort both by ascending value index\n",
    "    # and store the cumulative sum\n",
    "    sorted_indices = np.argsort(val)\n",
    "    sorted_risk = risk[sorted_indices].cumsum()\n",
    "    population = np.arange(0, len(risk) + 1)/len(risk)\n",
    "    # Normalize risk\n",
    "    risk_norm = sorted_risk/sorted_risk.max()\n",
    "    # Insert 0 for each\n",
    "    risk_norm = np.insert(0., 1, risk_norm)\n",
    "    # Calculate gini\n",
    "    gini = np.abs((np.diff(population))*((risk_norm + np.roll(risk_norm, shift=1))[1:]/2) -\n",
    "                  (np.diff(population)*(population + np.roll(population, shift=1))[1:]/2)).sum()/.5\n",
    "    return gini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will sort until we expend our budget. We get these values\n",
    "# from the hma projects dataset for elevation projects\n",
    "# We will sample by 500K until 6 million (roughly 95th%ile)\n",
    "# then by 1M until 15 million (roughly 99th%ile)\n",
    "budgets_typ = np.arange(1e6, 6.1e6, 5e5)\n",
    "budgets_high = np.arange(7e6, 15.1e6, 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we sort from top to bottom\n",
    "h_sort_desc = ['npv_opt', 'avoid_rel_eal',\n",
    "               'rel_eal']\n",
    "\n",
    "# We loop through each scen_ddftype combo\n",
    "# to execute the code below so that we have a potentially\n",
    "# different set of elevated homes for each policy\n",
    "# for each scenario\n",
    "# Do not want to spend computation on the very high budgets\n",
    "# for Lower and Upper scenario. Evaluating rules on those\n",
    "# budgets is a kind of conclusion sensitivity check for\n",
    "# the main 'Mid' results\n",
    "for scen, sort_df in sort_dfs.items():\n",
    "    # We also want to write out the ordering and\n",
    "    # the allocations\n",
    "    elev_dict = {}\n",
    "    obj_dict = {}\n",
    "\n",
    "    haz_scen = scen.split('_')[0]\n",
    "    if haz_scen == 'Mid':\n",
    "        budgets = np.append(budgets_typ, budgets_high)\n",
    "    else:\n",
    "        budgets = budgets_typ\n",
    "\n",
    "    # Dict of sort keys to fd_id values\n",
    "    sort_dict = {}\n",
    "    # This is for community sorting, more explanation later\n",
    "    slack_dict = {}\n",
    "    \n",
    "    print('Scenario: ' + scen)\n",
    "\n",
    "    # Get the corresponding ensemble df\n",
    "    ens_df = ens_dfs[scen]\n",
    "    \n",
    "    # Loop through ascending columns and sort, store in dict\n",
    "    # We want to sort on the col, and give ties to lower\n",
    "    # valued structures\n",
    "    for col in h_sort_desc:\n",
    "        sort_dict[col] = sort_df.sort_values([col, 'val_s'],\n",
    "                                              ascending=[False, True]).index\n",
    "    \n",
    "    # Community based sorting\n",
    "    sort_c_df = sort_df.join(sovi_df, how='inner')\n",
    "    \n",
    "    # Columns for community sorting\n",
    "    c_sort_cols = ['sovi', 'lmi', 'ovb', 'cejst']\n",
    "    \n",
    "    # For disadvantaged communities, we will\n",
    "    # spend over half of our budget on the highest\n",
    "    # priority properties in that community\n",
    "    # then spend the remainder of our funds\n",
    "    # on any property not already elevated,\n",
    "    # regardless of where it is located.\n",
    "    # We prioritize based on net benefit.\n",
    "    for col in c_sort_cols:\n",
    "        # Households in the disadvantaged community\n",
    "        sort_disad = sort_c_df[sort_c_df[col] == True]\n",
    "        # Sort these by highest to lowest benefit, with\n",
    "        # ties going to lower valued houses\n",
    "        disad_pri = sort_disad.sort_values(['npv_opt', 'val_s'],\n",
    "                                             ascending=[False, True]).index.to_list()\n",
    "        # We want the community based sorting to be\n",
    "        # based on greatest to lowest benefits, subject to our\n",
    "        # community constraint\n",
    "        sort_dict[col] = sort_df.sort_values(['npv_opt', 'val_s'],\n",
    "                                               ascending=[False, True]).index.to_list()\n",
    "        # We spend over half of our budget on these homes\n",
    "        # before going into sort_dict[col]\n",
    "        slack_dict[col] = disad_pri\n",
    "    \n",
    "    # Loop through budgets and the keys in sort_dict\n",
    "    # Calculate the elev_inst cumulative sum and subset to\n",
    "    # the value just under the budget\n",
    "    # Then calculate all of the objective values\n",
    "    # Store in a dict of\n",
    "    # sort_key_budget keys to objectives values\n",
    "    for budget in budgets:\n",
    "        for sort_col, fd_id in sort_dict.items():\n",
    "            # Key for obj dict\n",
    "            obj_key = scen + '_' + sort_col + '_' + str(budget)\n",
    "            # Sort our df according to the rule at hand\n",
    "            sorted_df = sort_df.reindex(fd_id)\n",
    "    \n",
    "            # Calculate the cumulative sum of elev_inst\n",
    "            sorted_df['policy_cost'] = sorted_df['elev_invst'].cumsum()\n",
    "    \n",
    "            # We want to make sure the majority of our budget\n",
    "            # goes to homes in the prioritization area.\n",
    "            # This appears to be the way FEMA is tracking\n",
    "            # this, so we want to be consistent. \n",
    "            if sort_col in c_sort_cols:\n",
    "                # The way we can do this is by taking the homes in the\n",
    "                # prioritization area, and making sure that we\n",
    "                # spend just over half our budget on them.\n",
    "                # Then we take all the remaining homes, regardless\n",
    "                # of their prioritization status, in terms of\n",
    "                # top to bottom benefits. This is what is stored\n",
    "                # in sorted_df. So, we just need to make sure\n",
    "                # that we remove the houses elevated from\n",
    "                # slack_ids before going to the remaining\n",
    "                # properties.            \n",
    "                \n",
    "                # First we need to identify the homes that are in\n",
    "                # our prioritizaton area. This is the set of homes\n",
    "                # in our \"slack\" \n",
    "                slack_ids = slack_dict[sort_col]\n",
    "                pri_df = sorted_df[sorted_df['fd_id'].isin(slack_ids)]\n",
    "                \n",
    "                # Next we need to allocate half of our budget\n",
    "                # plus 500K (higher than our max elevaton cost ensures\n",
    "                # majority of expenses go to these homes) to homes in the\n",
    "                # prioritization area\n",
    "                budget_alloc = budget/2 + 500000\n",
    "                # We want running cost of homes we are prioritizing\n",
    "                pri_df['policy_cost'] = pri_df['elev_invst'].cumsum()\n",
    "                \n",
    "                # Now subset based on our budget allocation limit\n",
    "                elevated_sub = pri_df[pri_df['policy_cost'] <= budget_alloc]\n",
    "                \n",
    "                # Then we get the amount of budget we have left\n",
    "                slack = budget - elevated_sub['policy_cost'].max()\n",
    "                \n",
    "                # Now, for all homes in sorted_df that are not in\n",
    "                # elevated_sub, we subset based on slack\n",
    "                slack_df = sorted_df[~sorted_df['fd_id'].isin(elevated_sub['fd_id'])]\n",
    "                # Get the running cost\n",
    "                slack_df['policy_cost'] = slack_df['elev_invst'].cumsum()\n",
    "                \n",
    "                slack_elev = slack_df[slack_df['policy_cost'] <= slack]\n",
    "                \n",
    "                # And we also have to subset based on the majority\n",
    "                # of npv coming from our elevated_sub df\n",
    "                slack_ben_max = elevated_sub['npv_opt'].sum() \n",
    "                slack_elev['npv_check'] = slack_elev['npv_opt'].cumsum()\n",
    "                slack_elev_sub = slack_elev[(slack_elev['npv_check']\n",
    "                                             < slack_ben_max)]\n",
    "                slack_elev_sub = slack_elev_sub.drop(columns='npv_check')\n",
    "                \n",
    "                # Now concat\n",
    "                elevated = pd.concat([elevated_sub, slack_elev_sub], axis=0)\n",
    "                \n",
    "            # If not community sorting, you just go through the sorted\n",
    "            # dataframe and subset subject to your budget\n",
    "            else:\n",
    "                elevated = sorted_df[sorted_df['policy_cost'] <= budget]\n",
    "    \n",
    "            # Calculated objectives\n",
    "            # We do this by finding the subset in ens_df that are elevated\n",
    "            # and the subset that are not. That's where we calculate\n",
    "            # our objectives for within each SOW, and then we take\n",
    "            # the expected values across the SOWs\n",
    "            elev_ens = ens_df[ens_df['fd_id'].isin(elevated['fd_id'])]\n",
    "            orig_ens = ens_df[~ens_df['fd_id'].isin(elevated['fd_id'])]\n",
    "    \n",
    "            # For npv, we calculate the sum of npv of elevated homes\n",
    "            npvs = elev_ens.groupby('sow_ind')['npv_opt'].sum()\n",
    "            # Our objective value for this policy is the mean of that\n",
    "            npv = np.mean(npvs)\n",
    "            \n",
    "            # For up_cost, we calculate the sum of elev_invst \n",
    "            up_costs = elev_ens.groupby('sow_ind')['elev_invst'].sum()\n",
    "            # Then we take the mean\n",
    "            up_cost = np.mean(up_costs)\n",
    "    \n",
    "            # Get the pv resid based on the whole set\n",
    "            # of homes with risk (there are benefits out of scope\n",
    "            # of our npv calculation which could be associated\n",
    "            # with lowering pv of residual risk, so we want\n",
    "            # policies that balance the npv of elevation while\n",
    "            # also not leaving more residual risk than needed)\n",
    "            \n",
    "            # We calculate the sum of pv_resid for elev_ens\n",
    "            # And we calculate the sum of pv_base for orig_ens\n",
    "            # Since these are indexed on sow_ind, we can add the\n",
    "            # two series. That's our resids, then we take the\n",
    "            # mean for our resid objective value for this policy\n",
    "            resid_elev = elev_ens.groupby('sow_ind')['pv_resid'].sum()\n",
    "            resid_orig = orig_ens.groupby('sow_ind')['pv_base'].sum()\n",
    "            resids = resid_elev + resid_orig\n",
    "            resid = np.mean(resids)\n",
    "    \n",
    "            # Gini index between residual relative risk and structure\n",
    "            # value. For elevated homes, we want to use their\n",
    "            # reid_rel_eal. For homes that are not elevated, \n",
    "            # we want to use rel_eal.\n",
    "            # We want to evaluate this for each SOW\n",
    "            # and then take the average of the vaues\n",
    "            \n",
    "            # Add a column\n",
    "            # with resid_rel_eal and rel_eal\n",
    "            # for the corresponding homes based on elevation\n",
    "            ens_df['remain_rel_eal'] = np.where(ens_df[\"fd_id\"].isin(elevated['fd_id']),\n",
    "                                                ens_df[\"resid_rel_eal\"],\n",
    "                                                ens_df[\"rel_eal\"])\n",
    "\n",
    "            # SOW specific gini coefficients\n",
    "            ginis = ens_df.groupby('sow_ind').apply(lambda x: risk_val_gini(x['remain_rel_eal'],\n",
    "                                                                            x['val_s']))\n",
    "            # Ensemble mean\n",
    "            gini = np.mean(ginis)\n",
    "\n",
    "            # Finally, there are a variety of metrics on rel_eal\n",
    "            # in the orig_ens dataframe\n",
    "            ens_r_gb = orig_ens.groupby('sow_ind')\n",
    "            # The avoid_eq objective can be calculated multiple ways\n",
    "            # There is min the max resid risk burden across houses\n",
    "            # There is min the median resid risk burden across houses\n",
    "            # There is min the sum of resid risk burden across houses\n",
    "            # There is min the 95hth %ile of resid risk burden \"\"\n",
    "            # We can calculate it each way to show\n",
    "            # there is some sensitivity about conclusions\n",
    "            # based on how you calculate things\n",
    "            avoid_eqs1 = ens_r_gb['rel_eal'].max()\n",
    "            avoid_eq1 = np.mean(avoid_eqs1)\n",
    "\n",
    "            avoid_eqs2 = ens_r_gb['rel_eal'].median()\n",
    "            avoid_eq2 = np.mean(avoid_eqs2)\n",
    "\n",
    "            avoid_eqs3 = ens_r_gb['rel_eal'].sum()\n",
    "            avoid_eq3 = np.mean(avoid_eqs3)\n",
    "\n",
    "            avoid_eqs4 = ens_r_gb['rel_eal'].quantile(.95)\n",
    "            avoid_eq4 = np.mean(avoid_eqs4)\n",
    "    \n",
    "            # Store objectives in dict\n",
    "            obj_dict[obj_key] = (npv, resid, up_cost,\n",
    "                                 avoid_eq1,\n",
    "                                 avoid_eq2,\n",
    "                                 avoid_eq3,\n",
    "                                 avoid_eq4,\n",
    "                                 gini)\n",
    "    \n",
    "            # Need to store the fd_id that end up in elevated in a dict\n",
    "            elev_dict[obj_key] = elevated['fd_id'].astype(int).to_list()\n",
    "    \n",
    "            print('Calculate objective values for policy:\\n'+\n",
    "                  'Sort by ' + sort_col + '\\nWith Budget of $M ' + str(budget))\n",
    "            \n",
    "    # Get the dataframe of objectives\n",
    "    # Need to have scen_policy then split into scen & policy columns\n",
    "    objs = pd.DataFrame.from_dict(obj_dict).T.reset_index()\n",
    "    objs.columns =  ['scen_policy', 'npv', 'pv_resid', 'up_cost',\n",
    "                    'avoid_eq1', 'avoid_eq2', 'avoid_eq3', 'avoid_eq4',\n",
    "                    'resid_eq']\n",
    "    objs['policy'] = objs['scen_policy'].str.split('_').str[1:].apply(lambda x: '_'.join(x))\n",
    "    objs['scen'] = objs['scen_policy'].str.split('_').str[:1].apply(lambda x: '_'.join(x))\n",
    "    objs['sort'] = objs['policy'].str.split('_').str[:-1].apply(lambda x: '_'.join(x))\n",
    "    objs['budget'] = objs['policy'].str.split('_').str[-1].astype(float).astype(int)\n",
    "\n",
    "    # Add a community vs. household indicator\n",
    "    objs.loc[objs['sort'].isin(c_sort_cols), 'res'] = 'community'\n",
    "    objs.loc[~objs['sort'].isin(c_sort_cols), 'res'] = 'household'\n",
    "\n",
    "    # Write out the dataframe of objective values\n",
    "    # and the dictionary of policy to fd_ids that are\n",
    "    # elevated\n",
    "    obj_filep = join(FO, 'pol_obj_vals_' + scen + '.pqt')\n",
    "    objs.to_parquet(obj_filep)\n",
    "\n",
    "    elev_ids_filep = join(FO, 'pol_elev_ids_' + scen + '.json')\n",
    "    with open(elev_ids_filep, 'w') as fp:\n",
    "        json.dump(elev_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gc_elev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
